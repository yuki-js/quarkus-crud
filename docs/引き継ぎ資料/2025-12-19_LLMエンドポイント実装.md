# LLMエンドポイント実装 - 引き継ぎ資料

**作成日**: 2025-12-19  
**対象**: LLMエンドポイント `/api/llm/fake-names` の実装

## 実装概要

プロフィールに対する似たような名前を提案し、クイズの選択肢として提示するためのLLMエンドポイントを実装しました。

### 実装内容

1. **OpenAPI仕様の追加**
   - `openapi/paths/llm.yaml`: `/api/llm/fake-names` エンドポイントの定義
   - `openapi/components/schemas/llm.yaml`: リクエスト/レスポンススキーマの定義
   - `openapi/openapi.yaml`: LLMタグとパス、スキーマの参照を追加

2. **依存関係の追加**
   - `build.gradle`: Quarkus LangChain4j Azure OpenAI extension (`io.quarkiverse.langchain4j:quarkus-langchain4j-azure-openai:0.21.0`)

3. **アプリケーション設定**
   - `src/main/resources/application.properties`:
     - Azure OpenAI Service接続設定（環境変数で上書き可能）
     - プロパティ: `quarkus.langchain4j.azure-openai.api-key`, `quarkus.langchain4j.azure-openai.endpoint`, `quarkus.langchain4j.azure-openai.chat-model.deployment-name`
   - `src/test/resources/application.properties`:
     - テスト用のダミー設定

4. **実装クラス（レイヤー構造に従う）**
   - **Resource層** (`LlmApiImpl.java`): HTTPリクエスト/レスポンス処理、認証、メトリクス
   - **UseCase層** (`LlmUseCase.java`): ビジネスロジックのオーケストレーション、バリデーション、DTO変換
   - **Service層**:
     - `LlmService.java`: LLM呼び出しとプロンプト処理
     - `RateLimiterService.java`: レート制限機能（ユーザーごと毎分100件、全体で毎分300件）

## エンドポイント仕様

### POST `/api/llm/fake-names`

**認証**: 必須（JWT Bearer Token）

**リクエスト**:
```json
{
  "inputName": "青木 勇樹",
  "variance": 0.1
}
```

- `inputName`: 元となる名前（必須、1-100文字）
- `variance`: 多様度（0.0-1.0、0=似た名前、1=多様な名前）

**レスポンス**:
```json
{
  "output": [
    "青木 優香",
    "青木 優空",
    "青山 裕子",
    "青木 雄",
    "青木 悠斗"
  ]
}
```

- 最低5つ以上の一意な名前を返す

**エラーレスポンス**:
- `400`: 無効なリクエストパラメータ
- `401`: 認証が必要
- `429`: レート制限超過
- `500`: サーバーエラー

## レート制限

インメモリの変数で管理:
- **ユーザーごと**: 毎分100リクエスト
- **グローバル**: 毎分300リクエスト

実装: `RateLimiterService.java`

## アーキテクチャ

本実装は既存のレイヤー構造に従っています:

1. **Resource層** (`resource/LlmApiImpl.java`)
   - HTTPリクエスト/レスポンスの処理
   - 認証（`@Authenticated`）
   - メトリクス収集（Micrometer）
   - エラーハンドリングとHTTPステータスコードのマッピング

2. **UseCase層** (`usecase/LlmUseCase.java`)
   - ビジネスロジックのオーケストレーション
   - リクエストのバリデーション
   - レート制限のチェック
   - DTOの変換とマッピング
   - ビジネス例外の定義（`RateLimitExceededException`）

3. **Service層** (`service/`)
   - `LlmService.java`: LLM APIの呼び出しとプロンプト処理
   - `RateLimiterService.java`: レート制限のロジック
   - ドメインロジックの実装

この構造により、各層の責務が明確に分離され、保守性とテスト容易性が向上しています。

## プロンプトテンプレート

`LlmService.java` 内に埋め込まれたプロンプトテンプレートを使用:
- 日本語の指示文
- JSON Schemaによる出力形式の指定
- 変数: `{{input_name}}`, `{{variance}}`
- 出力形式: `{ "output": ["名前1", "名前2", ...] }`

## 環境変数の設定

### ローカル開発環境

本番環境では以下の環境変数を設定してください:

```bash
# Azure OpenAI Service
export AZURE_OPENAI_API_KEY="your-api-key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_DEPLOYMENT_NAME="gpt-4"
```

**重要**: Azure OpenAI の場合、`AZURE_OPENAI_ENDPOINT` にはベースリソースURL（例: `https://your-resource.openai.azure.com/`）を指定してください。デプロイメントパスは含めません。拡張機能が自動的に正しいパスを構築します。

### Kubernetesデプロイメント

Kubernetes環境では、以下のファイルを更新してください：

1. **`manifests/application.properties`** - LLM設定を追加済み
2. **`manifests/common-secret.yaml`** - Azure OpenAI認証情報を追加:
   ```yaml
   stringData:
     AZURE_OPENAI_API_KEY: your-azure-openai-api-key
     AZURE_OPENAI_ENDPOINT: https://your-resource.openai.azure.com/
     AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4
   ```

**注意**: `AZURE_OPENAI_ENDPOINT` はベースリソースURLのみを含め、デプロイメントパスやAPIバージョンは含めないでください。

詳細は `docs/deployment.md` を参照してください。

## テスト

### テスト構成

**47の包括的なテストを実装:**

1. **統合テスト** (`LlmIntegrationTest` - 12テスト)
   - **WireMock サーバー使用**: OpenAI APIを完全にモック
   - `OpenAiMockServerResource` を使用して実HTTPリクエストでテスト
   - 認証、レート制限、バリデーション、エラーハンドリングをカバー

2. **ユースケース層テスト** (`LlmUseCaseTest` - 14テスト)
   - `@InjectMock` でサービス層をモック
   - ビジネスロジック、バリデーション、DTO変換をテスト

3. **サービス層テスト** (`LlmServiceTest` - 13テスト)
   - `ChatLanguageModel` をモック
   - プロンプト生成、JSONパース、エラーハンドリングをテスト

4. **レート制限テスト** (`RateLimiterServiceTest` - 8テスト)
   - レート制限ロジックの単体テスト

### OpenAI モックサーバー

テスト環境では、実際のOpenAI APIの代わりに **WireMock サーバー** を使用:

- **実装**: `OpenAiMockServerResource` (QuarkusTestResourceLifecycleManager)
- **動作**: 
  - テスト実行時に自動的にWireMockサーバーを起動
  - OpenAI API のエンドポイント (`/chat/completions`) をモック
  - 設定を上書きして `quarkus.langchain4j.openai.base-url` をモックサーバーに向ける
- **利点**:
  - 実際のHTTPリクエストをテスト（より現実的な統合テスト）
  - 実際のOpenAI APIへの呼び出しなし（コスト削減、高速、安定）
  - レスポンスをテストごとに柔軟に設定可能

### テスト実行方法

```bash
# すべてのテストを実行
./gradlew test

# LLM関連のテストのみ実行
./gradlew test --tests "*Llm*Test" --tests "*RateLimiter*Test"

# 統合テストのみ実行
./gradlew test --tests "LlmIntegrationTest"
```

## CI/CD

すべてのCI検証をパス:
- ✅ OpenAPI仕様のコンパイル
- ✅ Spectral検証（既存の警告/エラーは変更なし）
- ✅ OpenAPI Generator検証
- ✅ Gradle build
- ✅ Spotless (コードフォーマット)
- ✅ Checkstyle (コード品質)
- ✅ JUnit tests

## 注意事項

1. **本番デプロイ前の確認事項**:
   - Azure OpenAI Serviceのリソースとデプロイメント名を確認
   - APIキーとエンドポイントを環境変数で設定
   - レート制限の設定が要件に合っているか確認

2. **コスト管理**:
   - LLM APIの呼び出しにはコストがかかります
   - レート制限により過度な使用を防止していますが、モニタリングを推奨

3. **エラーハンドリング**:
   - LLM APIの呼び出し失敗時は500エラーを返します
   - メトリクス（`api.llm.fake_names.success`, `api.llm.fake_names.error`, `api.llm.rate_limit_exceeded`）を監視

4. **拡張性**:
   - 将来的に `/api/llm/{prompt-template-id}` の形式で他のプロンプトテンプレートを追加予定
   - プロンプトテンプレートの管理方法を検討（DB、設定ファイル、etc.）

## 次のステップ

- [ ] 本番環境でのAzure OpenAI Service設定
- [ ] モニタリングとアラートの設定
- [ ] 実際のユースケースでの動作確認
- [ ] 追加のプロンプトテンプレートの実装（必要に応じて）
- [ ] レート制限の調整（実際の使用状況に基づいて）

## 参考資料

- [Quarkus LangChain4j Documentation](https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html)
- [Azure OpenAI Service Documentation](https://learn.microsoft.com/azure/ai-services/openai/)
- プロンプトテンプレート: `src/main/java/app/aoki/quarkuscrud/service/LlmService.java`
