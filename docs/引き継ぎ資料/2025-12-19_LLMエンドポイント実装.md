# LLMエンドポイント実装 - 引き継ぎ資料

**作成日**: 2025-12-19  
**対象**: LLMエンドポイント `/api/llm/fake-names` の実装

## 実装概要

プロフィールに対する似たような名前を提案し、クイズの選択肢として提示するためのLLMエンドポイントを実装しました。

### 実装内容

1. **OpenAPI仕様の追加**
   - `openapi/paths/llm.yaml`: `/api/llm/fake-names` エンドポイントの定義
   - `openapi/components/schemas/llm.yaml`: リクエスト/レスポンススキーマの定義
   - `openapi/openapi.yaml`: LLMタグとパス、スキーマの参照を追加

2. **依存関係の追加**
   - `build.gradle`: Quarkus LangChain4j OpenAI extension (`io.quarkiverse.langchain4j:quarkus-langchain4j-openai:0.21.0`)

3. **アプリケーション設定**
   - `src/main/resources/application.properties`:
     - Azure OpenAI Service接続設定（環境変数で上書き可能）
     - プロパティ: `quarkus.langchain4j.openai.api-key`, `quarkus.langchain4j.openai.base-url`, `quarkus.langchain4j.openai.chat-model.model-name`
   - `src/test/resources/application.properties`:
     - テスト用のダミー設定

4. **実装クラス**
   - `RateLimiterService.java`: レート制限機能（ユーザーごと毎分100件、全体で毎分300件）
   - `LlmService.java`: LLM呼び出しとプロンプト処理
   - `LlmApiImpl.java`: REST APIエンドポイントの実装

## エンドポイント仕様

### POST `/api/llm/fake-names`

**認証**: 必須（JWT Bearer Token）

**リクエスト**:
```json
{
  "inputName": "青木 勇樹",
  "variance": 0.1
}
```

- `inputName`: 元となる名前（必須、1-100文字）
- `variance`: 多様度（0.0-1.0、0=似た名前、1=多様な名前）

**レスポンス**:
```json
{
  "output": [
    "青木 優香",
    "青木 優空",
    "青山 裕子",
    "青木 雄",
    "青木 悠斗"
  ]
}
```

- 最低5つ以上の一意な名前を返す

**エラーレスポンス**:
- `400`: 無効なリクエストパラメータ
- `401`: 認証が必要
- `429`: レート制限超過
- `500`: サーバーエラー

## レート制限

インメモリの変数で管理:
- **ユーザーごと**: 毎分100リクエスト
- **グローバル**: 毎分300リクエスト

実装: `RateLimiterService.java`

## プロンプトテンプレート

`LlmService.java` 内に埋め込まれたプロンプトテンプレートを使用:
- 日本語の指示文
- JSON Schemaによる出力形式の指定
- 変数: `{{input_name}}`, `{{variance}}`
- 出力形式: `{ "output": ["名前1", "名前2", ...] }`

## 環境変数の設定

本番環境では以下の環境変数を設定してください:

```bash
# Azure OpenAI Service
export AZURE_OPENAI_API_KEY="your-api-key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/v1"
export AZURE_OPENAI_DEPLOYMENT_NAME="gpt-4"
```

注: Azure OpenAI Serviceの場合、`base-url` には `/v1` サフィックスを含めてください。

## テスト

- すべての既存テストが成功
- LLM APIは既存のテストでは実際には呼び出されない（ダミー設定使用）
- 統合テストを追加する場合は、LLMサービスをモックするか、実際のAPIキーを使用

## CI/CD

すべてのCI検証をパス:
- ✅ OpenAPI仕様のコンパイル
- ✅ Spectral検証（既存の警告/エラーは変更なし）
- ✅ OpenAPI Generator検証
- ✅ Gradle build
- ✅ Spotless (コードフォーマット)
- ✅ Checkstyle (コード品質)
- ✅ JUnit tests

## 注意事項

1. **本番デプロイ前の確認事項**:
   - Azure OpenAI Serviceのリソースとデプロイメント名を確認
   - APIキーとエンドポイントを環境変数で設定
   - レート制限の設定が要件に合っているか確認

2. **コスト管理**:
   - LLM APIの呼び出しにはコストがかかります
   - レート制限により過度な使用を防止していますが、モニタリングを推奨

3. **エラーハンドリング**:
   - LLM APIの呼び出し失敗時は500エラーを返します
   - メトリクス（`api.llm.fake_names.success`, `api.llm.fake_names.error`, `api.llm.rate_limit_exceeded`）を監視

4. **拡張性**:
   - 将来的に `/api/llm/{prompt-template-id}` の形式で他のプロンプトテンプレートを追加予定
   - プロンプトテンプレートの管理方法を検討（DB、設定ファイル、etc.）

## 次のステップ

- [ ] 本番環境でのAzure OpenAI Service設定
- [ ] モニタリングとアラートの設定
- [ ] 実際のユースケースでの動作確認
- [ ] 追加のプロンプトテンプレートの実装（必要に応じて）
- [ ] レート制限の調整（実際の使用状況に基づいて）

## 参考資料

- [Quarkus LangChain4j Documentation](https://docs.quarkiverse.io/quarkus-langchain4j/dev/index.html)
- [Azure OpenAI Service Documentation](https://learn.microsoft.com/azure/ai-services/openai/)
- プロンプトテンプレート: `src/main/java/app/aoki/quarkuscrud/service/LlmService.java`
